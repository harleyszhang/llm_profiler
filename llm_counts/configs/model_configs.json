{
    "llama-7B":{
        "num_layers": 32,
        "num_heads": 32,
        "hidden_size": 4096,
        "intermediate_size": 11008,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "model_type": "llama",
        "model_name": "llama-7B"
    },
    "llama-13B":{
        "num_layers": 40,
        "num_heads": 40,
        "hidden_size": 5120,
        "intermediate_size": 13824,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "model_type": "llama",
        "model_name": "llama-13B"
    },
    "llama-30B":{
        "num_layers": 60,
        "num_heads": 52,
        "hidden_size": 6656,
        "intermediate_size": 17920,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "model_type": "llama",
        "model_name": "llama-30B"
    },
    "llama-65B":{
        "num_layers": 80,
        "num_heads": 64,
        "hidden_size": 8192,
        "intermediate_size": 22016,
        "vocab_size": 32000,
        "max_seq_len": 2048,
        "model_type": "llama",
        "model_name": "llama-65B"
    },
    "llama2-13B":{
        "num_layers": 40,
        "num_heads": 40,
        "num_kv_heads": 40,
        "hidden_size": 5120,
        "intermediate_size": 13824,
        "vocab_size": 32000,
        "max_seq_len": 4096,
        "model_type": "llama",
        "model_name": "llama2-13B"
    },
    "llama2-70B":{
        "num_layers": 80,
        "num_heads": 64,
        "num_kv_heads": 8,
        "hidden_size": 8192,
        "intermediate_size": 28672,
        "vocab_size": 32000,
        "max_seq_len": 4096,
        "model_type": "llama2",
        "model_name": "llama2-70B"
    },
    "internlm-20B": {
        "num_layers": 60,
        "num_heads": 40,
        "num_kv_heads": 40,
        "hidden_size": 5120,
        "intermediate_size": 13824,
        "vocab_size": 103168,
        "max_seq_len": 16384,
        "model_type": "internlm",
        "model_name": "internlm-20B"
    },
    "internlm2-20b-chat": {
        "num_layers": 48,
        "num_heads": 48,
        "num_kv_heads": 8,
        "hidden_size": 6144,
        "intermediate_size": 16384,
        "vocab_size": 92544,
        "max_seq_len": 32768,
        "model_type": "internlm2",
        "model_name": "internlm2-20b-chat"
    },
    "Qwen3-8B": {
        "num_layers": 36,
        "head_dim": 128,
        "hidden_size": 4096,
        "num_heads": 32,
        "num_kv_heads": 8,
        "intermediate_size": 12288,
        "vocab_size": 151936,
        "max_seq_len": 40960,
        "model_type": "qwen3",
        "model_name": "Qwen3-8B"
    },
    "Qwen3-32B": {
        "num_layers": 64,
        "head_dim": 128,
        "hidden_size": 5120,
        "num_heads": 64,
        "num_kv_heads": 8,
        "intermediate_size": 25600,
        "vocab_size": 151936,
        "max_seq_len": 40960,
        "model_type": "qwen3",
        "model_name": "Qwen3-32B"
    },
    "Qwen3-30B-A3B": {
        "num_layers": 48,
        "head_dim": 128,
        "hidden_size": 2048,
        "num_heads": 32,
        "num_kv_heads": 4,
        "intermediate_size": 6144,
        "moe_intermediate_size": 768,
        "vocab_size": 151936,
        "max_seq_len": 40960,
        "num_experts": 128,
        "num_experts_per_tok": 8,
        "model_type": "qwen3_moe",
        "model_name": "Qwen3-30B-A3B"
    },
    "Qwen3-235B-A22B": {
        "num_layers": 94,
        "head_dim": 128,
        "hidden_size": 4096,
        "num_heads": 64,
        "num_kv_heads": 4,
        "intermediate_size": 12288,
        "moe_intermediate_size": 1536,
        "vocab_size": 151936,
        "max_seq_len": 40960,
        "num_experts": 128,
        "num_experts_per_tok": 8,
        "model_type": "qwen3_moe",
        "model_name": "Qwen3-235B-A22B"
    }
}
